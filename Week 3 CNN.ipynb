{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2604976b",
   "metadata": {},
   "source": [
    "# Histopathologic Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7610a",
   "metadata": {},
   "source": [
    "## Martin Ozaeta\n",
    "\n",
    "https://github.com/MOzaeta96/Week-3-CNN-project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93780595",
   "metadata": {},
   "source": [
    "    For the Kaggle competition, we were tasked with identifying metastic cancer in small image patches from pathology scans.\n",
    "This was accomplished via a deduplicated version of the PatchCamelyon or PCam dataset. For out initial work we needed to set everything up for our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7860c",
   "metadata": {},
   "source": [
    "### Importing Libraries:\n",
    "\n",
    "    Our notebook thankfully has tensorflow, keras, opencv-python and matplotlib installed so we can move forward with importing the libraries we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0ec2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdbbc9e",
   "metadata": {},
   "source": [
    "    The libraries will allow us to manipulate data, process the images that are found in the data set, and create our model that will help with identifying the cancer cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2f616",
   "metadata": {},
   "source": [
    "### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f51b5c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image Tensor(\"args_0:0\", shape=(), dtype=string): Image at path Tensor(\"args_0:0\", shape=(), dtype=string) is empty or could not be read.\n",
      "Error loading image Tensor(\"args_0:0\", shape=(), dtype=string): Image at path Tensor(\"args_0:0\", shape=(), dtype=string) is empty or could not be read.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def load_image(image_path, label):\n",
    "    try:\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image, channels=3)\n",
    "        \n",
    "        # Check if image is empty\n",
    "        if tf.shape(image) == tf.constant([0]):\n",
    "            raise ValueError(f\"Image at path {image_path} is empty or could not be read.\")\n",
    "        \n",
    "        image = tf.image.resize(image, [128, 128])\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        image = tf.zeros([128, 128, 3])  # Create a dummy image if there's an error\n",
    "    return image, label\n",
    "\n",
    "def create_dataset_from_dataframe(df, base_path, batch_size=32):\n",
    "    image_paths = df['id'].apply(lambda x: os.path.join(base_path, x)).values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int64)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load the training labels CSV\n",
    "train_labels_df = pd.read_csv('C:\\\\Users\\\\marti\\\\Downloads\\\\histopathologic-cancer-detection\\\\train_labels.csv')\n",
    "\n",
    "# Update the file extension and path if necessary\n",
    "train_labels_df['id'] = train_labels_df['id'].apply(lambda x: x + '.tif')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(train_labels_df, test_size=0.2, stratify=train_labels_df['label'], random_state=42)\n",
    "\n",
    "# Base path to the training images\n",
    "train_base_path = 'C:\\\\Users\\\\marti\\\\Downloads\\\\histopathologic-cancer-detection\\\\train'\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 32\n",
    "train_dataset = create_dataset_from_dataframe(train_df, train_base_path, batch_size)\n",
    "val_dataset = create_dataset_from_dataframe(val_df, train_base_path, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba5249",
   "metadata": {},
   "source": [
    "### Creating the model and augmenting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e980ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model creation function\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = len(train_labels_df['label'].unique())  # Update with the number of classes\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf76d7c",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_augmentation_pipeline():\n",
    "    return ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ed419",
   "metadata": {},
   "source": [
    "### Training and Testing the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4c1489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m770s\u001b[0m 140ms/step - accuracy: 0.5952 - loss: 0.6770 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 2/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 135ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 3/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 130ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 4/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m718s\u001b[0m 131ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 5/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 131ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 6/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 130ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 7/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 128ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 8/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 127ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 9/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 134ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n",
      "Epoch 10/10\n",
      "\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 132ms/step - accuracy: 0.5952 - loss: 0.6750 - val_accuracy: 0.5950 - val_loss: 0.6750\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81fd6a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading image Tensor(\"args_0:0\", shape=(), dtype=string): Image at path Tensor(\"args_0:0\", shape=(), dtype=string) is empty or could not be read.\n"
     ]
    }
   ],
   "source": [
    "test_base_path = 'C:\\\\Users\\\\marti\\\\Downloads\\\\histopathologic-cancer-detection\\\\test'  # Update with the actual path\n",
    "\n",
    "# List all files in the test directory\n",
    "test_image_files = [f for f in os.listdir(test_base_path) if f.endswith('.tif')]\n",
    "\n",
    "# Create DataFrame\n",
    "test_df = pd.DataFrame({\n",
    "    'id': test_image_files\n",
    "})\n",
    "\n",
    "# Optional: Add a placeholder label column if needed\n",
    "test_df['label'] = -1\n",
    "\n",
    "def create_test_dataset(df, base_path, batch_size=32):\n",
    "    image_paths = df['id'].apply(lambda x: os.path.join(base_path, x)).values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    dataset = dataset.map(lambda x: load_image(x, -1), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "test_dataset = create_test_dataset(test_df, test_base_path, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582107c",
   "metadata": {},
   "source": [
    "### Create Predictions and prepare data for submission to Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73234a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 25ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_classes = predictions.argmax(axis=-1)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "test_df['label'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c301121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Compress into ZIP file\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
    "    zipf.write('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ff939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
